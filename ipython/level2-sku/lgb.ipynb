{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import date\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "order = pd.read_csv(\"../../data/level2/m111-order-sku.csv\", sep=',', parse_dates=['order_date'])\n",
    "dis = pd.read_csv(\"../../data/level2/m111-dis-sku.csv\", sep=',', parse_dates=['dis_date'])\n",
    "inv = pd.read_csv(\n",
    "    \"../../data/level2/m111-inv-sku.csv\", sep=',', parse_dates=['period_wid']\n",
    ").rename(columns={'period_wid': 'inv_date'})\n",
    "category = pd.read_csv(\n",
    "    \"../../data/level2/m111-item-category.csv\", sep=','\n",
    ").rename(columns={'sales_segment1_code': 'category'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 处理订单数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取2017年至2018年区间中的数据\n",
    "order = order.loc[(order['order_date'] >= '2017-01-01') & (order['order_date'] <= '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_month = order.copy()\n",
    "order_month['month'] = order_month['order_date'].astype('str').apply(lambda x: x[:7])\n",
    "order_month = order_month.groupby(['item_code', 'month'])[['qty']].sum()\n",
    "order_month = order_month.unstack(level=-1).fillna(0)\n",
    "order_month.columns = pd.date_range('2017-01-31', '2018-12-31', freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 处理分销数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取2017年至2018年区间中的数据（分销数据只有从3月份开始的数据且3月份不完整）>_<|||\n",
    "dis = dis.loc[(dis['dis_date'] >= '2017-04-01') & (dis['dis_date'] <= '2018-12-31')]\n",
    "\n",
    "# 处理分销量为负数的情况\n",
    "dis['qty'] = dis['qty'].apply(lambda x: -x if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_month = dis.copy()\n",
    "dis_month['month'] = dis_month['dis_date'].astype('str').apply(lambda x: x[:7])\n",
    "dis_month = dis_month.groupby(['item_code', 'month'])[['qty']].sum()\n",
    "dis_month = dis_month.unstack(level=-1).fillna(0)\n",
    "dis_month.columns = pd.date_range('2017-04-30', '2018-12-31', freq='M')\n",
    "dis_month = dis_month.reindex(order_month.index).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 处理库存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取2017年至2018年区间中的数据（库存数据只有从2017年6月份开始的数据）>_<|||\n",
    "inv = inv.loc[(inv['inv_date'] >= '2017-06-01') & (inv['inv_date'] <= '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除异常值\n",
    "inv = inv.loc[~(inv.qty > 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取每月的最后一天作为当月的库存\n",
    "inv = inv.loc[inv['inv_date'].isin(pd.date_range('2017-06-30', '2018-12-31', freq='M'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_month = inv.copy()\n",
    "inv_month['month'] = inv_month['inv_date'].astype('str').apply(lambda x: x[:7])\n",
    "inv_month = inv_month.groupby(['item_code', 'month'])[['qty']].sum()\n",
    "inv_month = inv_month.unstack(level=-1).fillna(0)\n",
    "inv_month.columns = pd.date_range('2017-06-30', '2018-12-31', freq='M')\n",
    "inv_month = inv_month.reindex(order_month.index).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 处理品类信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = pd.read_csv(\n",
    "    \"../../data/level2/m111-item-category.csv\", sep=','\n",
    ").rename(columns={'sales_segment1_code': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas的bug：'21054110000024', '21054110000025', '21054110000084', '21054110000085' 这4个字符串居然是重复的，取消注释即可看到\n",
    "# category.loc[category.item_code.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决上面bug的下策，避免下面reindex报错\n",
    "category.drop_duplicates(['item_code'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = category.set_index('item_code').reindex(order_month.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "category['category'] = encoder.fit_transform(category['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 得到每个品类每个月的提货数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_cate_month = order_month.reset_index()\n",
    "order_cate_month['category'] = category['category'].values\n",
    "order_cate_month = order_cate_month.groupby('category')[order_month.columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 得到每个品类每个月的分销数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_cate_month = dis_month.reset_index()\n",
    "dis_cate_month['category'] = category['category'].values\n",
    "dis_cate_month = dis_cate_month.groupby('category')[dis_month.columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 得到每个品类每个月的库存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cate_month = inv_month.reset_index()\n",
    "inv_cate_month['category'] = category['category'].values\n",
    "inv_cate_month = inv_cate_month.groupby('category')[inv_month.columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(order, dis, inv, year, month, is_train=True, name_prefix=None):\n",
    "    X = {}\n",
    "    \n",
    "    # 提货的统计特征（28个特征）\n",
    "    for i in [3, 6, 9, 12]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = order[pd.date_range(end=dt, periods=i, freq='M')]  # 前i个月提货量\n",
    "        X['ord_diff_mean_pre_%s' % i] = tmp.diff(axis=1).mean(axis=1).values  # 前i个月提货量的平均一阶差分\n",
    "        X['ord_sum_decay_pre_%s' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values  # 前i个月提货量的和（带衰减）\n",
    "        X['ord_mean_pre_%s' % i] = tmp.mean(axis=1).values  # 前i个月提货量的平均值\n",
    "        X['ord_median_pre_%s' % i] = tmp.median(axis=1).values  # 前i个月提货量的中位数\n",
    "        X['ord_max_pre_%s' % i] = tmp.max(axis=1).values  # 前i个月提货量的最大值\n",
    "        X['ord_min_pre_%s' % i] = tmp.min(axis=1).values  # 前i个月提货量的最小值\n",
    "        X['ord_std_pre_%s' % i] = tmp.std(axis=1).values  # 前i个月提货量的标准差\n",
    "        \n",
    "    # 分销的统计特征（21个特征）>_<|||\n",
    "    for i in [3, 6, 9]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = dis[pd.date_range(end=dt, periods=i, freq='M')]  # 前i个月分销量\n",
    "        X['dis_diff_mean_pre_%s' % i] = tmp.diff(axis=1).mean(axis=1).values  # 前i个月分销量的平均一阶差分\n",
    "        X['dis_sum_decay_pre_%s' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values  # 前i个月分销量的和（带衰减）\n",
    "        X['dis_mean_pre_%s' % i] = tmp.mean(axis=1).values  # 前i个月分销量的均值\n",
    "        X['dis_median_pre_%s' % i] = tmp.median(axis=1).values  # 前i个月分销量的中位数\n",
    "        X['dis_max_pre_%s' % i] = tmp.max(axis=1).values  # 前i个月分销量的最大值\n",
    "        X['dis_min_pre_%s' % i] = tmp.min(axis=1).values  # 前i个月分销量的最小值\n",
    "        X['dis_std_pre_%s' % i] = tmp.std(axis=1).values  # 前i个月分销量的标准差\n",
    "        \n",
    "    # 库存的统计特征（14个）>_<|||\n",
    "    for i in [3, 6]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = inv[pd.date_range(end=dt, periods=i, freq='M')]  # 前i个月库存量\n",
    "        X['inv_diff_mean_pre_%s' % i] = tmp.diff(axis=1).mean(axis=1).values  # 前i个月库存量的平均一阶差分\n",
    "        X['inv_sum_decay_pre_%s' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values  # 前i个月库存量的和（带衰减）\n",
    "        X['inv_mean_pre_%s' % i] = tmp.mean(axis=1).values  # 前i个月库存量的均值\n",
    "        X['inv_median_pre_%s' % i] = tmp.median(axis=1).values  # 前i个月库存量的中位数\n",
    "        X['inv_max_pre_%s' % i] = tmp.max(axis=1).values  # 前i个月库存量的最大值\n",
    "        X['inv_min_pre_%s' % i] = tmp.min(axis=1).values  # 前i个月库存量的最小值\n",
    "        X['inv_std_pre_%s' % i] = tmp.std(axis=1).values  # 前i个月库存量的标准差\n",
    "        \n",
    "    # 提货天数特征（12个特征）\n",
    "    for i in [3, 6, 9, 12]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = order[pd.date_range(end=dt, periods=i, freq='M')]\n",
    "        X['has_ord_pre_%s' % i] = (tmp > 0).sum(axis=1).values  # 前i个月有提货的天数\n",
    "        X['last_ord_pre_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values  # 前i个月距离上一次有提货的天数\n",
    "        X['first_ord_pre_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values  # 前i个月距离第一次有提货的天数\n",
    "        \n",
    "    # 分销天数特征（9个特征）>_<|||\n",
    "    for i in [3, 6, 9]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = dis[pd.date_range(end=dt, periods=i, freq='M')]\n",
    "        X['has_dis_pre_%s' % i] = (tmp > 0).sum(axis=1).values  # 前i个月有分销的天数\n",
    "        X['last_dis_pre_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values  # 前i个月距离上一次有分销的天数\n",
    "        X['first_dis_pre_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values  # 前i个月距离第一次有分销的天数\n",
    "        \n",
    "    # 库存天数特征（6个特征）>_<|||\n",
    "    for i in [3, 6]:\n",
    "        dt = date(year, month, 1)\n",
    "        tmp = inv[pd.date_range(end=dt, periods=i, freq='M')]\n",
    "        X['has_inv_pre_%s' % i] = (tmp > 0).sum(axis=1).values  # 前i个月有库存的天数\n",
    "        X['last_inv_pre_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values  # 前i个月距离上一次有库存的天数\n",
    "        X['first_inv_pre_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values  # 前i个月距离第一次有库存的天数\n",
    "        \n",
    "    # 前12个月的提货量\n",
    "    for i in range(1, 13):\n",
    "        if month - i <= 0:\n",
    "            start_dt = date(year - 1, month + 12 - i, 1)\n",
    "        else:\n",
    "            start_dt = date(year, month - i, 1)\n",
    "        X['ord_pre_%s' % i] = order[pd.date_range(start_dt, periods=1, freq='M')].values.ravel()\n",
    "        \n",
    "    # 前9个月的分销量>_<|||\n",
    "    for i in range(1, 10):\n",
    "        if month - i <= 0:\n",
    "            start_dt = date(year - 1, month + 12 - i, 1)\n",
    "        else:\n",
    "            start_dt = date(year, month - i, 1)\n",
    "        X['dis_pre_%s' % i] = dis[pd.date_range(start_dt, periods=1, freq='M')].values.ravel()\n",
    "       \n",
    "    # 前6个月的库存量>_<|||\n",
    "    for i in range(1, 7):\n",
    "        if month - i <= 0:\n",
    "            start_dt = date(year - 1, month + 12 - i, 1)\n",
    "        else:\n",
    "            start_dt = date(year, month - i, 1)\n",
    "        X['inv_pre_%s' % i] = inv[pd.date_range(start_dt, periods=1, freq='M')].values.ravel()\n",
    "        \n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    if is_train:\n",
    "        start_dt = date(year, month, 1)\n",
    "        y = order[pd.date_range(start_dt, periods=3, freq='M')].values\n",
    "        return X, y\n",
    "    \n",
    "    if name_prefix is not None:\n",
    "        X.columns = ['%s_%s' % (name_prefix, c) for c in X.columns]\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 准备训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_month = [ \n",
    "    '2018-01', \n",
    "    '2018-02', \n",
    "    '2018-03', \n",
    "    '2018-04'\n",
    "]\n",
    "\n",
    "X_l, y_l = [], []\n",
    "for month in train_month:\n",
    "    y, m = int(month.split('-')[0]), int(month.split('-')[1])\n",
    "    \n",
    "    X_tmp, y_tmp = prepare_dataset(order_month, dis_month, inv_month, y, m)\n",
    "    \n",
    "    X_tmp2 = prepare_dataset(order_cate_month, dis_cate_month, inv_cate_month, y, m, is_train=False, name_prefix='cate')\n",
    "    X_tmp2.index = order_cate_month.index\n",
    "    X_tmp2 = X_tmp2.reindex(category['category']).reset_index(drop=True)\n",
    "    \n",
    "    X_tmp = pd.concat([X_tmp, X_tmp2, category.reset_index(drop=True)], axis=1)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "    del X_tmp, y_tmp, X_tmp2\n",
    "    gc.collect()\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 准备验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_dataset(order_month, dis_month, inv_month, 2018, 7)\n",
    "\n",
    "X_val2 = prepare_dataset(order_cate_month, dis_cate_month, inv_cate_month, 2018, 7, is_train=False, name_prefix='cate')\n",
    "X_val2.index = order_cate_month.index\n",
    "X_val2 = X_val2.reindex(category['category']).reset_index(drop=True)\n",
    "\n",
    "X_val = pd.concat([X_val, X_val2, category.reset_index(drop=True)], axis=1)\n",
    "\n",
    "del X_val2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 准备测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prepare_dataset(order_month, dis_month, inv_month, 2018, 10)\n",
    "\n",
    "X_test2 = prepare_dataset(order_cate_month, dis_cate_month, inv_cate_month, 2018, 10, is_train=False, name_prefix='cate')\n",
    "X_test2.index = order_cate_month.index\n",
    "X_test2 = X_test2.reindex(category['category']).reset_index(drop=True)\n",
    "\n",
    "X_test = pd.concat([X_test, X_test2, category.reset_index(drop=True)], axis=1)\n",
    "\n",
    "del X_test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training and predicting...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Start training and predicting...\")\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[50]\ttraining's l2: 1.49004e+07\tvalid_1's l2: 4.94339e+06\n",
      "[100]\ttraining's l2: 1.31539e+07\tvalid_1's l2: 4.29263e+06\n",
      "[150]\ttraining's l2: 1.24224e+07\tvalid_1's l2: 4.13895e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1040: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's l2: 1.2026e+07\tvalid_1's l2: 4.08059e+06\n",
      "[250]\ttraining's l2: 1.17823e+07\tvalid_1's l2: 4.05905e+06\n",
      "[300]\ttraining's l2: 1.15978e+07\tvalid_1's l2: 4.05203e+06\n",
      "[350]\ttraining's l2: 1.14472e+07\tvalid_1's l2: 4.09448e+06\n",
      "[400]\ttraining's l2: 1.13261e+07\tvalid_1's l2: 4.07144e+06\n",
      "Early stopping, best iteration is:\n",
      "[289]\ttraining's l2: 1.16393e+07\tvalid_1's l2: 4.02805e+06\n",
      "ord_sum_decay_pre_3: 350461174974.59\n",
      "ord_pre_2: 305572057171.00\n",
      "ord_pre_4: 143435768983.00\n",
      "ord_mean_pre_3: 138232301429.80\n",
      "ord_pre_1: 75743007038.00\n",
      "ord_min_pre_3: 56662486431.00\n",
      "ord_pre_7: 56361873764.00\n",
      "ord_max_pre_9: 54734720754.90\n",
      "ord_sum_decay_pre_9: 52295064771.61\n",
      "ord_max_pre_6: 49004801445.83\n",
      "ord_sum_decay_pre_6: 42270128618.50\n",
      "ord_mean_pre_6: 31456503104.54\n",
      "ord_max_pre_3: 30778020526.70\n",
      "ord_mean_pre_9: 29905669758.11\n",
      "ord_median_pre_6: 25284898095.00\n",
      "ord_max_pre_12: 24266505673.24\n",
      "ord_pre_12: 24154193449.31\n",
      "ord_std_pre_6: 22855236127.31\n",
      "ord_pre_10: 22342258583.54\n",
      "ord_std_pre_12: 20779408712.71\n",
      "ord_mean_pre_12: 17119590347.13\n",
      "ord_diff_mean_pre_3: 9533408579.90\n",
      "ord_pre_3: 5472773273.00\n",
      "ord_min_pre_6: 4760528879.00\n",
      "ord_pre_9: 4645541881.35\n",
      "ord_sum_decay_pre_12: 4075202516.77\n",
      "ord_min_pre_12: 4015293276.00\n",
      "ord_pre_5: 3592599154.60\n",
      "ord_median_pre_9: 3059418201.00\n",
      "ord_diff_mean_pre_9: 2560832642.06\n",
      "ord_diff_mean_pre_6: 2328050209.10\n",
      "ord_median_pre_12: 1901732119.70\n",
      "ord_median_pre_3: 1704891879.00\n",
      "ord_std_pre_3: 1671104559.00\n",
      "ord_std_pre_9: 1476930400.24\n",
      "ord_pre_11: 1355750169.50\n",
      "ord_min_pre_9: 1083380840.00\n",
      "ord_diff_mean_pre_12: 1034313525.17\n",
      "ord_pre_6: 987430555.95\n",
      "ord_pre_8: 260420524.60\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[50]\ttraining's l2: 7.65603e+06\tvalid_1's l2: 9.77283e+06\n",
      "[100]\ttraining's l2: 6.63855e+06\tvalid_1's l2: 8.11975e+06\n",
      "[150]\ttraining's l2: 6.245e+06\tvalid_1's l2: 7.41201e+06\n",
      "[200]\ttraining's l2: 6.0238e+06\tvalid_1's l2: 7.02423e+06\n",
      "[250]\ttraining's l2: 5.88241e+06\tvalid_1's l2: 6.78663e+06\n",
      "[300]\ttraining's l2: 5.78051e+06\tvalid_1's l2: 6.61942e+06\n",
      "[350]\ttraining's l2: 5.69379e+06\tvalid_1's l2: 6.53101e+06\n",
      "[400]\ttraining's l2: 5.62122e+06\tvalid_1's l2: 6.48832e+06\n",
      "[450]\ttraining's l2: 5.55137e+06\tvalid_1's l2: 6.39637e+06\n",
      "[500]\ttraining's l2: 5.48824e+06\tvalid_1's l2: 6.37178e+06\n",
      "[550]\ttraining's l2: 5.42844e+06\tvalid_1's l2: 6.3271e+06\n",
      "[600]\ttraining's l2: 5.37548e+06\tvalid_1's l2: 6.30358e+06\n",
      "[650]\ttraining's l2: 5.32829e+06\tvalid_1's l2: 6.27135e+06\n",
      "[700]\ttraining's l2: 5.2867e+06\tvalid_1's l2: 6.25418e+06\n",
      "[750]\ttraining's l2: 5.2436e+06\tvalid_1's l2: 6.22738e+06\n",
      "[800]\ttraining's l2: 5.2039e+06\tvalid_1's l2: 6.22795e+06\n",
      "[850]\ttraining's l2: 5.16707e+06\tvalid_1's l2: 6.21065e+06\n",
      "[900]\ttraining's l2: 5.13003e+06\tvalid_1's l2: 6.20115e+06\n",
      "[950]\ttraining's l2: 5.09306e+06\tvalid_1's l2: 6.16734e+06\n",
      "[1000]\ttraining's l2: 5.05737e+06\tvalid_1's l2: 6.13233e+06\n",
      "[1050]\ttraining's l2: 5.02657e+06\tvalid_1's l2: 6.1251e+06\n",
      "[1100]\ttraining's l2: 4.99494e+06\tvalid_1's l2: 6.11136e+06\n",
      "[1150]\ttraining's l2: 4.96637e+06\tvalid_1's l2: 6.1033e+06\n",
      "[1200]\ttraining's l2: 4.93781e+06\tvalid_1's l2: 6.12166e+06\n",
      "[1250]\ttraining's l2: 4.90742e+06\tvalid_1's l2: 6.07816e+06\n",
      "[1300]\ttraining's l2: 4.87872e+06\tvalid_1's l2: 6.02834e+06\n",
      "[1350]\ttraining's l2: 4.85234e+06\tvalid_1's l2: 6.03676e+06\n",
      "[1400]\ttraining's l2: 4.82502e+06\tvalid_1's l2: 6.02582e+06\n",
      "[1450]\ttraining's l2: 4.79684e+06\tvalid_1's l2: 6.03315e+06\n",
      "[1500]\ttraining's l2: 4.77237e+06\tvalid_1's l2: 6.02382e+06\n",
      "[1550]\ttraining's l2: 4.74987e+06\tvalid_1's l2: 5.99836e+06\n",
      "[1600]\ttraining's l2: 4.7233e+06\tvalid_1's l2: 5.97238e+06\n",
      "[1650]\ttraining's l2: 4.69851e+06\tvalid_1's l2: 5.95162e+06\n",
      "[1700]\ttraining's l2: 4.67562e+06\tvalid_1's l2: 5.94222e+06\n",
      "[1750]\ttraining's l2: 4.65263e+06\tvalid_1's l2: 5.93366e+06\n",
      "[1800]\ttraining's l2: 4.63099e+06\tvalid_1's l2: 5.9252e+06\n",
      "[1850]\ttraining's l2: 4.60714e+06\tvalid_1's l2: 5.92422e+06\n",
      "[1900]\ttraining's l2: 4.58772e+06\tvalid_1's l2: 5.93289e+06\n",
      "[1950]\ttraining's l2: 4.56547e+06\tvalid_1's l2: 5.93349e+06\n",
      "Early stopping, best iteration is:\n",
      "[1865]\ttraining's l2: 4.60122e+06\tvalid_1's l2: 5.90292e+06\n",
      "ord_pre_1: 338439535993.80\n",
      "ord_min_pre_3: 128876658469.20\n",
      "ord_sum_decay_pre_3: 111351044398.00\n",
      "ord_max_pre_9: 75483235191.20\n",
      "ord_mean_pre_9: 64769286124.97\n",
      "ord_pre_6: 62289805007.07\n",
      "ord_pre_3: 46878876491.50\n",
      "ord_pre_2: 36801316243.40\n",
      "ord_std_pre_12: 35931141742.80\n",
      "ord_median_pre_6: 32021114555.70\n",
      "ord_median_pre_3: 31157143651.80\n",
      "ord_pre_9: 30817822027.11\n",
      "ord_max_pre_3: 28959746531.78\n",
      "ord_sum_decay_pre_9: 27114822604.56\n",
      "ord_diff_mean_pre_12: 25050678647.09\n",
      "ord_std_pre_6: 23477863057.97\n",
      "ord_mean_pre_3: 21848127163.69\n",
      "ord_max_pre_12: 19961309526.54\n",
      "ord_pre_4: 19527591407.61\n",
      "ord_max_pre_6: 19425669668.67\n",
      "ord_diff_mean_pre_9: 18456580979.77\n",
      "ord_std_pre_3: 16584083428.19\n",
      "ord_pre_11: 14112430012.67\n",
      "ord_diff_mean_pre_3: 13988023972.11\n",
      "ord_pre_8: 13828419516.95\n",
      "ord_mean_pre_6: 13185921598.74\n",
      "ord_pre_5: 13090288382.60\n",
      "ord_diff_mean_pre_6: 12481409824.51\n",
      "ord_sum_decay_pre_12: 11772189041.87\n",
      "ord_std_pre_9: 11476211072.15\n",
      "ord_mean_pre_12: 9376296612.86\n",
      "ord_sum_decay_pre_6: 9172559243.73\n",
      "ord_pre_7: 8810464693.86\n",
      "ord_median_pre_12: 7995573476.18\n",
      "ord_median_pre_9: 6300963394.79\n",
      "ord_pre_10: 5814078953.05\n",
      "ord_min_pre_6: 5346182532.00\n",
      "ord_min_pre_9: 4498798110.00\n",
      "ord_pre_12: 4115475204.55\n",
      "ord_min_pre_12: 2674909696.00\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[50]\ttraining's l2: 7.97623e+06\tvalid_1's l2: 1.87864e+07\n",
      "[100]\ttraining's l2: 7.06903e+06\tvalid_1's l2: 1.64726e+07\n",
      "[150]\ttraining's l2: 6.69084e+06\tvalid_1's l2: 1.53914e+07\n",
      "[200]\ttraining's l2: 6.47257e+06\tvalid_1's l2: 1.48937e+07\n",
      "[250]\ttraining's l2: 6.31902e+06\tvalid_1's l2: 1.46002e+07\n",
      "[300]\ttraining's l2: 6.2003e+06\tvalid_1's l2: 1.44015e+07\n",
      "[350]\ttraining's l2: 6.10037e+06\tvalid_1's l2: 1.42592e+07\n",
      "[400]\ttraining's l2: 6.01082e+06\tvalid_1's l2: 1.42016e+07\n",
      "[450]\ttraining's l2: 5.92539e+06\tvalid_1's l2: 1.41686e+07\n",
      "[500]\ttraining's l2: 5.84358e+06\tvalid_1's l2: 1.40867e+07\n",
      "[550]\ttraining's l2: 5.77385e+06\tvalid_1's l2: 1.39918e+07\n",
      "[600]\ttraining's l2: 5.71395e+06\tvalid_1's l2: 1.39537e+07\n",
      "[650]\ttraining's l2: 5.65773e+06\tvalid_1's l2: 1.39261e+07\n",
      "[700]\ttraining's l2: 5.60097e+06\tvalid_1's l2: 1.38665e+07\n",
      "[750]\ttraining's l2: 5.54765e+06\tvalid_1's l2: 1.38754e+07\n",
      "[800]\ttraining's l2: 5.50173e+06\tvalid_1's l2: 1.38222e+07\n",
      "[850]\ttraining's l2: 5.4562e+06\tvalid_1's l2: 1.37498e+07\n",
      "[900]\ttraining's l2: 5.40921e+06\tvalid_1's l2: 1.37809e+07\n",
      "[950]\ttraining's l2: 5.36835e+06\tvalid_1's l2: 1.37719e+07\n",
      "Early stopping, best iteration is:\n",
      "[867]\ttraining's l2: 5.44038e+06\tvalid_1's l2: 1.37345e+07\n",
      "ord_pre_2: 158525153516.20\n",
      "ord_mean_pre_9: 93606336255.91\n",
      "ord_mean_pre_3: 80293750418.00\n",
      "ord_sum_decay_pre_3: 60188924355.11\n",
      "ord_mean_pre_6: 60059489282.68\n",
      "ord_max_pre_6: 53942502905.88\n",
      "ord_pre_1: 52303620038.30\n",
      "ord_min_pre_3: 50292180421.00\n",
      "ord_pre_10: 48477907720.93\n",
      "ord_median_pre_6: 46377370931.90\n",
      "ord_std_pre_12: 37957973349.26\n",
      "ord_median_pre_12: 27047562024.70\n",
      "ord_sum_decay_pre_6: 26953395254.56\n",
      "ord_pre_4: 24047167789.60\n",
      "ord_max_pre_9: 22081194967.59\n",
      "ord_diff_mean_pre_3: 20118108762.00\n",
      "ord_sum_decay_pre_12: 19598268262.15\n",
      "ord_pre_6: 18727280804.80\n",
      "ord_pre_3: 16809128506.60\n",
      "ord_median_pre_9: 15744846747.90\n",
      "ord_pre_5: 15581922730.61\n",
      "ord_diff_mean_pre_12: 15528447285.69\n",
      "ord_max_pre_3: 14029221207.69\n",
      "ord_median_pre_3: 13916550735.10\n",
      "ord_std_pre_6: 11975518347.85\n",
      "ord_pre_12: 10485209951.39\n",
      "ord_pre_7: 10314669094.31\n",
      "ord_sum_decay_pre_9: 10152024623.21\n",
      "ord_mean_pre_12: 9978839295.75\n",
      "ord_max_pre_12: 9901782050.95\n",
      "ord_pre_8: 8655109514.14\n",
      "ord_std_pre_3: 5161651188.99\n",
      "ord_pre_9: 5113737426.59\n",
      "ord_std_pre_9: 4139593546.11\n",
      "ord_diff_mean_pre_6: 3889063656.00\n",
      "ord_min_pre_9: 3512089582.00\n",
      "ord_min_pre_6: 2974566065.00\n",
      "ord_diff_mean_pre_9: 2891339842.00\n",
      "ord_pre_11: 1101420258.80\n",
      "ord_min_pre_12: 861019968.00\n",
      "[INFO] Finished! ( ^ _ ^ ) V\n",
      "[INFO] Done in 98.949384 seconds.\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'num_leaves': 80, \n",
    "    'objective': 'regression', \n",
    "    'min_data_in_leaf': 200, \n",
    "    'learning_rate': 0.02, \n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.7, \n",
    "    'bagging_freq': 1, \n",
    "    'metric': 'l2', \n",
    "    'num_threads': 16\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 5000\n",
    "pred_val = []\n",
    "pred_test = []\n",
    "cate_vars = []\n",
    "\n",
    "for i in range(3):\n",
    "    print('=' * 50)\n",
    "    print(\"Step %d\" % (i + 1))\n",
    "    print('=' * 50)\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train[:, i], categorical_feature=cate_vars)\n",
    "    dval = lgb.Dataset(X_val, label=y_val[:, i], reference=dtrain, categorical_feature=cate_vars)\n",
    "    \n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS, \n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=125, verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    feat_imp = [(\"%s: %.2f\" % x) for x in sorted(zip(X_train.columns, bst.feature_importance('gain')), key=lambda x: x[1], reverse=True)]\n",
    "    print('\\n'.join(feat_imp))\n",
    "    pred_val.append(\n",
    "        bst.predict(X_val, num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "    )\n",
    "    pred_test.append(\n",
    "        bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "    )\n",
    "    \n",
    "print(\"[INFO] Finished! ( ^ _ ^ ) V\")\n",
    "print(\"[INFO] Done in %f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 7888494.379996519\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation mse:\", mean_squared_error(y_val, np.array(pred_val).transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
